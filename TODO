truncated bptt??
embed words before feeding to NN
init weights to random==>don't forget seeds
specify places that you can do W[x] instead of W*x
read comments
what is decoder LSTM equation?
change all dims and axes to -1(or -n)
decoder initial states

hidden size of the last decoder layer os VOCAB_SIZE but others is HIDDEN_SIZE

how tensorflow parallelizes

GPU

object oriented

visualization

other training methods(Adam)==>use what google has done

self graph in addition to weights

I suppose that the output of the decoder at layer 8 is used as decoder layer 1 input: if yes==>parallelism, if no==> what the input is?

change reshape to expand_dimes

do not use scan==>use only one matrix multiplication


everywhere I use np.zeros or sth==>use float32

when I extend, extend with EOS or nothing(all 0 in the one=hot vector)

I dont use corpus.sizes now. I should use it to trim the end EOS when all od the sentences have same size in a batch.

how can we go from prob to loss (maximum likelihood)

remove print('tmp1') and etc.